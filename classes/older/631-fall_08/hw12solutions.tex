\documentclass[10pt]{article}

\usepackage[margin=1in, head=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{braket}

% if using a MAC, you may want to uncomment the following line
% to enable reverse searches.
\usepackage{pdfsync}

% Headers and footers
\fancyhf{}
\rfoot{\thepage}

%\setcounter{secnumdepth}{0}

% macros for algebra class
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\b}{\textbf}
\newcommand{\re}{\text{Re}}
\newcommand{\im}{\text{Im}}
\renewcommand{\iff}{\Leftrightarrow}
\newcommand{\zbar}{\overline{z}}
\newcommand\SL{\operatorname{SL}}
\newcommand\GL{\operatorname{GL}}
\newcommand{\divides}{\, \Big | \,}

\newcommand{\normsubeq}{\trianglelefteq}
\newcommand{\normsub}{\triangleleft}
\newcommand{\gen}[1]{\left\langle #1 \right\rangle}
\newcommand{\sect}[1]{\vspace{.25in}\noindent\textbf{Section #1}}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\zmod}[1]{\Z/#1 \Z}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand\inv{^{-1}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\renewcommand{\char}{\text{ char }}
\newcommand{\Syl}{\operatorname{Syl}}
%\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\Span}{\operatorname{Span}}

\newcommand{\tor}{\operatorname{Tor}}
\newcommand{\noun}[1]{\textsc{#1}}
\newcommand{\F}{\mathbb{F}}

\parindent=0in
\parskip=0.5\baselineskip

% LOOK HERE
% change assignment number and possibly date below
\newcommand\header{{\sc Math 631 \hfill Homework 12 \hfill December 5, 2008}}

\begin{document}

\header

\section*{Section 10.2}

\begin{itemize}

\item[3.]  Give an explicit example of a map from one $R$-module to another which is a group homomorphism but not an $R$-module homomorphism.

Example:(Buchholz)

Let $R=\Z[i]$ and view $R$ as a module over itself.  Let $\varphi:R\rightarrow R$ where $\varphi(a+bi)=a.$  For $\varphi$ to be an $R$-module homomorphism $\varphi$ must be operation preserving with respect to addition and have the property that $r\varphi(r)=\varphi(r*r)$ for all $r\in R$.    

Now consider the elements $x= a_1 + b_1i$, and $y = a_2 + b_2 i \in R.$  We have 
$$\varphi(x+y)=a_1 + a_2 = \varphi(x)+\varphi(y).$$
Hence $\varphi$ is operation preserving with respect to addition.  But,
$$\varphi(i*i)=\varphi(-1)=-1$$
and 
$$i\varphi(i)=i(0)=0.$$
Since $-1\neq 0$, $\varphi$ is not an $R$-module homomorphism.  However, 
$\varphi$ is a group homomorphism  since $\varphi$ is operation preserving with respect to addition.

\item[5.] Exhibit all $\Z$-module homomorphisms from $\Z/30\Z$ to $\Z/21\Z$.

(Bastille) Note that $(30,21)=3$, so by Exercise 10.2.6,
$$\text{Hom}_{\Z}(\Z/30\Z,\Z/21\Z)=\left\{\sigma_k: \ \Z/30\Z \to \Z/21\Z \ | \ \sigma_k(\bar{1})=7k+21\Z , 0 \leq k <3 \right\}. $$
Concretely, there are 3 distinct $\Z$-module homomorphisms, entirely determined by:
$$ \sigma_1(\bar{1})=\bar{7}, \qquad \sigma_2(\bar{1})=\overline{14}, \qquad \sigma_3(\bar{1})=\bar{0}. $$

\item[6.] Prove that $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z) \cong \Z/(n,m)\Z$.

\begin{proof} (Bastille) Consider $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ as an Abelian group under addition. Note that $\sigma \in \text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ if and only if the following applies:
\begin{enumerate}
\item[(a)] $\sigma: \ \Z/n\Z \to \Z/m\Z$;
\item[(b)] $\sigma(\bar{b}+\bar{c})=\sigma(\bar{b})+\sigma(\bar{c})$ for all $\bar{b}, \bar{c} \in \Z/n\Z$; since $\la \bar{1} \ra=\Z/n\Z$, note that satisfying condition (b) implies that $\sigma$ is entirely determined by $\sigma(\bar{1})$;
\item[(c)] $\sigma$ is well-defined, i.e. in this case, we must have $\sigma(\bar{b})=\sigma(\bar{c})$ whenever $\bar{b}=\bar{c}$.
\end{enumerate}
We claim that $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)=S$ where
$$S:=\left\{\sigma_k: \ \Z/n\Z \to \Z/m\Z \ | \ \sigma_k \text{ satisfies (b) and } \sigma_k(\bar{1})=k\dfrac{m}{(n,m)}+m\Z , \ k\in \Z, \ 0 \leq k < (n,m) \right\}.$$
Denote $\bar{a}=\sigma(\bar{1})$. Since there are only $m$ distinct cosets in $\Z/m\Z$, we can assume WLOG $0 \leq a <m$. If $\sigma \in \text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ then $\sigma$ is well-defined, so in particular, since $\bar{0}=\bar{n}$ in $\Z/n\Z$, we must have in $\Z/m\Z$: $\bar{0}=\sigma(\bar{0})=\sigma(\bar{n})=\sigma(n\bar{1})=n\sigma(\bar{1})=n\bar{a}.$ Hence $na=qm$ for some $q \in \Z$. Now we also have $n=(n,m)k_1$ for some $0 < k_1 \leq n$, and hence $a= \frac{q}{k_1}\frac{m}{(n,m)}$. Since $\frac{m}{(n,m)} \in \Z$, it must be that $\frac{q}{k_1}$ is also an integer. Furthermore since $0 \leq a < m$, it follows that $a=k\frac{m}{(n,m)}$ for $k$ an integer such that $0 \leq k <(n,m)$. Hence $\sigma \in S$ and $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z) \subseteq S$ \ \ (1).

Now if $\sigma_k \in S$, conditions (a) and (b) are met so we need only show that $\sigma_k$ is well-defined. Assume $\bar{b},\bar{c} \in \Z/n\Z$ such that $\bar{b}=\bar{c}$. Then $b=c+np$ for some $p \in \Z$ and
\begin{align*}
\sigma_k(\bar{b})&= \sigma_k(b\bar{1})=b\sigma_k(\bar{1})=(c+np)\left(k\frac{m}{(n,m)}+m\Z\right) = (c+np)k \frac{m}{(n,m)}+m\Z\\
&=ck\frac{m}{(n,m)}+npk\frac{m}{(n,m)}+m\Z= ck \frac{m}{(n,m)}+k_1pkm+m\Z=ck\frac{m}{(n,m)}+m\Z\\
&=c\left(k\frac{m}{(n,m)}+m\Z\right)=c\sigma_k(\bar{1})=\sigma_k(\bar{c}).
\end{align*}
Therefore $\sigma_k \in \text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ and $S \subseteq \text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ (2). Combining (1) and (2) leads to our claim. Now clearly $\sigma_k \neq \sigma_{k^{'}}$ whenever $k \neq k^{'}$ where $0 \leq k,k^{'} <(n,m)$, so
$$\left|\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)\right|=(n,m).$$
Note also that $\sigma_1$ generates $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ since for any $0 \leq k <(n,m)$, $\sigma_k=k\sigma_1$.
Therefore as an additive group, $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z)$ is Abelian and cyclic of order $(n,m)$ so $\text{Hom}_{\Z}(\Z/n\Z,\Z/m\Z) \cong \Z/(n,m)\Z$ and the isomorphism carries over to $\Z$-modules (since multiplication by an element of $\Z$ can be written as an addition), and so we have the desired result.
\end{proof}

{\sc Comment:}  There are other ways to do this, though perhaps less concrete.
Think about the way we talked about this problem before class last week.

\end{itemize}

\section*{Section 11.1}

\begin{itemize}

\item[1.]  Let $V=\R^{n}$ and let $(a_{1},a_{2},\cdots,a_{n})$
be a fixed vector in $V$. Prove that the collection $D$ of elements
$(x_{1},x_{2},\cdots,x_{n})\in V$ where $a_{1}x_{1}+a_{2}x_{2}+\cdots+a_{n}x_{n}=0$
is a subspace of $V$. Find the dimension and a basis of this subspace.

\begin{proof} (Allman)
For ease, let ${\bf a} = (a_1, \cdots, a_n) \in V$.  If ${\bf a} = {\bf 0}$,
then $D = V$ and $\dim_\R(D) = n$.  (Every vector in $V$ is orthogonal to
the zero vector.)

Now suppose that ${\bf a} \neq {\bf 0}$, and define $W = \Span({\bf a})$.
Since ${\bf a} \in V$, $W$ is a vector sub-space of $V$.  Note that $\dim(W) = 1$.  Define $W^\perp = D = \{v = (x_1, \dots, x_n) \in V \mid a_1 x_1 + \cdots 
+ a_n x_n = 0 \} = \{ v \in V \mid {\bf a} \cdot v = {\bf 0}\}.$
(We say $D$ is \emph{$W$ perp} meaning the collection of vectors perpendicular
to all vectors in $W$.)

{\sc Claim:}  $W^\perp = D$ is a vector subspace of $V$ of dimension $n-1$.

\smallskip

Clearly, the zero vector is an element of $W^\perp$.  Suppose $x$, $y \in
W^\perp$, then ${\bf a} \cdot (x + y) = {\bf a} \cdot x + {\bf a} \cdot y
= {\bf 0}$ and so the vector sum $x + y \in W^\perp$.  Let $r \in \R$ be
a scalar and $x \in W^\perp$, then ${\bf a} \cdot rx = r({\bf a} \cdot x)
= r {\bf 0} = {\bf 0}$.  Thus, $rx \in W^\perp$ and $W$ is a vector subspace
of $V$.

For the `Find' part of this problem, probably the most sophisticated way to proceed is
to argue that $V = W \oplus W^\perp$.  Notice if $w \in W \cap W^\perp$,
then ${\bf a} \cdot w = {\bf 0}$ and $w = r {\bf a}$ for some scalar
$r \in \R$.  Thus,
$$
{\bf a} \cdot w = {\bf a} \cdot r {\bf a}
= r ({\bf a} \cdot {\bf a})
= {\bf 0}.
$$
Now if $r = 0$, then $w$ is the zero vector.  If $r \neq 0$, then we must
have that ${\bf a} \cdot {\bf a} = 0$.  But this is a contradiction since only
the zero vector satisfies ${\bf a} \cdot {\bf a} = 0$. (Only the zero vector
has length zero.)  Thus, the only vector in the intersection $W \cap W^\perp$
is the zero vector.

To obtain a basis for $V^\perp$, start with a basis $\mathcal{B'} = \{v_1,
v'_2, \cdots, v'_n\}$ for
$V$ with the first basis vector given by $v_1 = {\bf a}$ and then extend
to a basis for $V$.
You can use Gram-Schmidt or just general ideas on projections to replace
the basis elements $v'_2, \dots, v'_n$ with $v_2, \dots, v_n$ which are
orthogonal to $v_1 = {\bf a}$.  Thus, $\mathcal{B} = \{v_1, v_2, \cdots,
v_n\}$ is a basis for $V = W \oplus W^\perp$.

More concretely, to show $\dim_\R(W^\perp) = n-1$ for ${\bf a} \neq
{\bf 0}$, we can without loss of generality assume that the last coordinate
$a_n$ of ${\bf a}$ is non-zero, $a_n \neq 0$.  Then for any choice of 
real numbers, $x_1, \cdots, x_{n-1}$, let $x_n =
\frac{-1}{a_n} (a_1 x_1 + \cdots + a_{n-1} x_{n-1})$ and $x = (x_1,
\dots, x_n)$.  It is easy to check that ${\bf a} \cdot x = 0$ and so $x \in W^\perp$.
Finally, since there were $n-1$ free choices $(x_1, \dots, x_{n-1})$, this means
that $\dim_\R(W^\perp) = n-1$.
\end{proof}


\item[2.] Let $V$ be the collection of polynomials
with coefficients in $\Q$ in the variable $x$ of degree at most
5. Prove that $V$ is a vector space over $\Q$ of dimension 6, with
$1,x.x^{2},\dots,x^{5}$ as a basis. Prove that $1,1+x,1+x+x^{2},\dots,1+x+x^{2}+x^{3}+x^{4}+x^{5}$
is also a basis for $V$.

\begin{proof} (Allman/Gillispie)
 By definition we know that $\{1,x,x^{2},\dots x^{5}\}=\mathcal{A}$
is a linearly independent subset of $\Q[x]$, which was shown to span
a vector space in e.g. 11.1.1.

Moreover, 
letting $p(x)\in V$, by definition $p(x)=p_{5}x^{5}+p_{4}x^{4}+p_{3}x^{3}+p_{2}x^{2}+p_{1}x+p_{0}$
where $p_{i}\in\Q$, we see that $\mathcal{A}$ spans $V$.  Thus, $\mathcal{A}$
is a basis for $V$.  Since $\vert \mathcal{A} \vert = 6$, $\dim_\Q(V) = 6$.

Let $\mathcal{B}=\{1,1+x,1+x+x^{2},\dots,1+x+x^{2}+x^{3}+x^{4}+x^{5}\}$.

Note that
\begin{align*}
1 &= 1\\
x &= -1(1) + 1(1+x)\\
x^2 &= -1(1+x) + 1(1+x+x^2)\\
&\vdots\\
x^5 &=-1(1+x+x^2+x^3+x^4) + 1(1+x+x^2+x^3+x^4+x^5).
\end{align*}
That is, each basis element in $\mathcal{A}$ can be expressed as
a linear combination of elements in $\mathcal{C}$ and vice versa.

It follows that $\mathcal{C}$ is also a basis for $V$.
\end{proof}



\item[3.] Let $\varphi$ be the linear transformation $\varphi:\R^4\rightarrow\R$ such that
\\ $\varphi((1,0,0,0)) = 1$ \ \ \ \ \ \ \ \ $\varphi((1,-1,0,0)) = 0$
\\ $\varphi((1,-1,1,0)) = 1$ \ \ \ \ \ \ $\varphi((1,-1,1,-1)) = 0$.
\\ Determine $\varphi((a,b,c,d))$.
\\
\\ (Baggett) \ We have that
\\ $-d(1,-1,1,-1) + (c+d)(1,-1,1,0) + (-b-c)(1,-1,0,0) + (a+b)(1,0,0,0)$\\
 $= (-d+c+d-b-c+a+b, d-c-d+b+c, -d+c+d, d)$\\
 $= (a, b, c, d)$.


Thus,
\\ $\varphi((a,b,c,d)) = \varphi( -d(1,-1,1,-1) + (c+d)(1,-1,1,0) + (b-c)(1,-1,0,0) + (a+b)(1,0,0,0) )$\\
 $= -d\varphi((1,-1,1,-1)) + (c+d)\varphi((1,-1,1,0)) + (-b-c)\varphi((1,-1,0,0)) + (a+b)\varphi((1,0,0,0))$\\
 $= -d(0) + (c+d)(1) + (-b-c)(0) + (a+b)(1)$\\
 $= a + b + c + d$.

\item[8.]
Let $V$ be a vector space over $F$ and let $\varphi$ be a linear transformation of the vector space $V$ to itself.  A nonzero element $v\in V$ satisfying $\varphi (v) = \lambda v$ for some $\lambda \in F$ is called an \textit{eigenvector} of $\varphi$ with \textit{eigenvalue $\lambda$}.  Prove that for any fixed $\lambda \in F$ the collection of eigenvectors of $\varphi$ with eigenvalue $\lambda$ together with $0$ forms a subspace of $V$.

\begin{proof}(Mobley) \ Let $W$ be the collection of eigenvectors of $\varphi$ with eigenvalue $\lambda$ together with $0$.  Let $x,y \in W$.  Also let $r \in F$.  Then for $(rx+y)\in V$, we have that $\varphi(rx+y)=\varphi(rx)+\varphi(y) = r \varphi(x) + \varphi(y)$, since $\varphi$ is a linear transformation of $V$.

Moreover, since $x$ and $y$ are eigenvectors, $r \varphi(x) + \varphi(y) =
r \lambda x + \lambda y = \lambda (rx + y) = \lambda \varphi(rx + y)$.
Hence, $(rx+y) \in W$ and $W$ is a subspace of $V$.
\end{proof}

\item[9.] Let $V$ be a vector space over $F$ and let $\phi$ be a linear transformation of the vector space $V$ to itself.  Suppose for $i = 1,2,\ldots, k$ that $v_i \in V$ is an eigenvector for $\phi$ with eigenvalue $\lambda_i \in F$ and that all the eigenvalues $\lambda_i$ are distinct.  Prove that $v_1,v_2,\ldots,v_k$ are linearly independent.  Conclude that any linear transformation on an $n$-dimensional vector space has at most $n$ distinct eigenvalues.

\begin{proof} (Schamel)
We shall proceed by induction on the number of distinct eigenvalues.  Note that when $k=1$, if $a_1 v_1 = 0$ for some $a_1 \in F$ then $a_1 = 0$ since $v_1 \neq 0$ and $F$ is a field.  Hence $\set{v_1}$ is linearly independent.  Suppose any $k$ eigenvectors of $\phi$ with distinct eigenvalues are linearly independent and consider the collection $\set{v_i}_{i=1}^{k+1}$ of eigenvectors of $\phi$ with distinct eigenvalues.  Suppose $\sum_{i=1}^{k+1} a_i v_i = 0$ for some $a_1,\ldots,a_{k+1} \in F$.  But then, applying $\phi$ to both sides of the equation yields $\sum_{i=1}^{k+1} a_i \lambda_i v_i = 0$ and we see
\[ 0 = \sum_{i=1}^{k+1} a_i \lambda_i v_i - \lambda_{k+1}\left(\sum_{i=1}^{k+1} a_i v_i\right) = \sum_{i=1}^k a_i (\lambda_i-\lambda_{k+1}) v_i. \]
However, since $\set{v_i}_{i=1}^k$ is linearly independent by hypothesis, we have $a_i (\lambda_i - \lambda_{k+1}) = 0$ for all $1 \leq i \leq k$.  Each eigenvalue is distinct, so $(\lambda_i - \lambda_{k+1}) \neq 0$ for all $1 \leq i \leq k$ and thus $a_i = 0$ for $1 \leq i \leq k$.  Going back to our original linear independence condition, we now have $a_{k+1}v_{k+1} = 0$, so $a_{k+1} = 0$ since $v_{k+1} \neq 0$.  We conclude $\set{v_i}_{i=1}^{k+1}$ is linearly independent and by induction any set of $k$ eigenvectors of $\phi$ with distinct eigenvalues is linearly independent.  Hence any linear transformation on an $n$-dimensional vector space has at most $n$ distinct eigenvalues, for otherwise it would contain a linearly independent set of vectors of order greater than $n$.
\end{proof}

\end{itemize}


\section*{Section 11.2}

\begin{itemize}

\item[1.] Let $V$ be the collection of polynomials with coefficients in $\Q$ in the variable $x$ of degree at most 5.  Determine the transition matrix from the basis $1,x,x^2\ldots, x^5$ for $V$ to the basis $1,1+x,1+x+x^2, \ldots, 1+x+x^2+x^3+x^4+x^5$ for $V$.

(Hazlett) 

Note, $1 = (1)1, 1+x =-1(1)+1(1+x), 1+x+x^2 = -1(1+x) + 1(1+x+x^2), \ldots,$ and $1+x+x^2+x^3+x^4+x^5 = -1(1+x+x^2+x^3+x^4) + 1(1+x+x^2+x^3+x^4+x^5)$.  Therefore, the transition matrix from the basis $1,x,x^2\ldots, x^5$ for $V$ to the basis $1,1+x,1+x+x^2, \ldots, 1+x+x^2+x^3+x^4+x^5$ will be:
$$\left(
\begin{matrix}
1&-1&0&0&0&0\\
0&1&-1&0&0&0\\
0&0&1&-1&0&0\\
0&0&0&1&-1&0\\
0&0&0&0&1&-1\\
0&0&0&0&0&1
\end{matrix}
\right)$$

\item[2.] (Lawless) Let $V$ be the vector space of the preceding exercise.  Let $\phi = d/dx$ be the linear transformation of $V$ to itself given by usual differentiation of a polynomial with respect to $x$.  Determine the matrix of $\phi$ with respect to the two bases for $V$ in the previous exercise. 
\\
\\
The differentiation matrix will have as its $i$th column the coordinates of the image if the $i$th basis element under the differentiation map. For example, $x^4 \mapsto 4x^3$, so the 5th column of the matrix with respect to the first basis will have a 4 in the 4th row, and zeros elsewhere. The other columns are defined similarly, as is the matrix with respect to the second basis. 


The matrix with respect to the first basis:
\begin{equation*} 
\left[
\begin{matrix}
0 & 1 & 0 & 0 & 0 & 0  \\
0 & 0 & 2 & 0 & 0 & 0  \\
0 & 0 & 0 & 3 & 0 & 0  \\
0 & 0 & 0 & 0 & 4 & 0  \\
0 & 0 & 0 & 0 & 0 & 5  \\
0 & 0 & 0 & 0 & 0 & 0  \\
\end{matrix}
\right]
\end{equation*}

The matrix with respect to the second basis:
\begin{equation*} 
\left[
\begin{matrix}
0 & 1 & -1& -1& -1& -1\\
0 & 0 & 2 & -1& -1& -1\\
0 & 0 & 0 & 3 & -1& -1\\
0 & 0 & 0 & 0 & 4 & -1\\
0 & 0 & 0 & 0 & 0 & 5 \\
0 & 0 & 0 & 0 & 0 & 0 \\
\end{matrix}
\right]
\end{equation*}


\item [4.] Let $\phi$ be the linear transformation of $\R^{2}$ to itself
given by rotation counterclockwise around the origin through an angle
$\theta$. Show that the matrix of $\phi$ with respect to the standard
basis for $\R^{2}$ is:\[
\left(\begin{matrix}\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta\end{matrix}\right)\]


\begin{proof} [Proof (Granade)]
Let $e_{1},e_{2}\in\R^{2}$ be the elements of the standard basis
$\mathcal{B}$. That is, let $e_{1},e_{2}$ have coordinates: $e_{1}=\left(\begin{matrix}1\\
0\end{matrix}\right)_{\mathcal{B}}$ and $e_{2}=\left(\begin{matrix}0\\
1\end{matrix}\right)_{\mathcal{B}}$. Then, $e_{1}$ has unit length and makes an angle of 0 with the $\hat{x}$-axis,
and so $\phi\left(e_{1}\right)=\left(\begin{matrix}\cos\theta\\
\sin\theta\end{matrix}\right)_{\mathcal{B}}$. Similarly, $e_{2}$ has unit length and makes an angle of $\pi/2$
with the $\hat{x}$-axis, and so $\phi\left(e_{2}\right)=\left(\begin{matrix}\cos\left(\theta+\pi/2\right)\\
\sin\left(\theta+\pi/2\right)\end{matrix}\right)_{\mathcal{B}}=\left(\begin{matrix}-\sin\theta\\
\cos\theta\end{matrix}\right)_{\mathcal{B}}$ by the sum of angle identities for sine and cosine:\begin{eqnarray*}
\cos\left(\theta+\frac{\pi}{2}\right) & = & \cos\theta\cos\frac{\pi}{2}-\sin\theta\sin\frac{\pi}{2}\\
 & = & 0\cos\theta-1\sin\theta\\
\sin\left(\theta+\frac{\pi}{2}\right) & = & \sin\theta\cos\frac{\pi}{2}+\cos\theta\sin\frac{\pi}{2}\\
 & = & 0\sin\theta+1\cos\theta\end{eqnarray*}
Thus, since the action of $\phi$ on the basis has been established,
we can write the matrix $\left(M_{\phi}\right)_{\mathcal{B}}$:\[
\left(\begin{matrix}\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta\end{matrix}\right)\]
\end{proof}


\end{itemize}



\end{document}
