\documentclass[10pt]{article}

\usepackage[margin=1in, head=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fancyhdr}
\usepackage{graphicx}

% if using a MAC, you may want to uncomment the following line
% to enable reverse searches.
\usepackage{pdfsync}

% Headers and footers
\fancyhf{}
\rfoot{\thepage}

%\setcounter{secnumdepth}{0}

% macros for algebra class
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\b}{\textbf}
\newcommand{\re}{\text{Re}}
\newcommand{\im}{\text{Im}}
\renewcommand{\iff}{\Leftrightarrow}
\newcommand{\zbar}{\overline{z}}
\newcommand\SL{\operatorname{SL}}
\newcommand\GL{\operatorname{GL}}
\newcommand{\divides}{\, \Big | \,}

\newcommand{\normsubeq}{\trianglelefteq}
\newcommand{\normsub}{\triangleleft}
\newcommand{\gen}[1]{\left\langle #1 \right\rangle}
\newcommand{\sect}[1]{\vspace{.25in}\noindent\textbf{Section #1}}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\zmod}[1]{\Z/#1 \Z}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand\inv{^{-1}}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\Inn}{\text{Inn}}
\renewcommand{\char}{\text{ char }}
\newcommand{\Syl}{\operatorname{Syl}}


\parindent=0in
\parskip=0.5\baselineskip

% LOOK HERE
% change assignment number and possibly date below
\newcommand\header{{\sc Math 631 \hfill Homework 8 \hfill October 31, 2008}}

\begin{document}

\header  


\section*{Section 7.1}

\begin{itemize}

\item[11.]  Prove that if $R$ is an integral domain and $x^2=1$ for some $x\in R$ then $x=\pm 1.$

\begin{proof}(Buchholz)
Let $x\in R$ where $x^2=1$.  Then
\begin{align*} 
x^2&=1\\
x^2-1&=0\\
(x-1)(x+1)&=0
\end{align*}
So $(x-1)=0$ or $(x+1)=0$, but $R$ was an integral domain which implies that $R$ has no zero divisors.  Therefore $x=1$ or $x=-1$.
\end{proof}

\item[13.]\emph{An element $x$ in $R$ is called {\bf
nilpotent} if $x^m=0$ for some $m\in \Z^+$.}

\emph{(a) Show that if $n=a^kb$ for some integers $a$ and $b$ then
$\overline{ab}$ is a nilpotent element of $\Z/n\Z$}
\begin{proof}
Let $n=a^kb$ for some $a,b\in \Z$ then
$$(ab)^k=(a^kb)b^{k-1}=nb^{k-1}$$
hence $(ab)^k\equiv 0\mod n$.
\end{proof}

\emph{(b) If $a\in \Z$ is integer, show that the element
$\bar{a}\in\Z/n\Z$ is a nilpotent if and only if every prime
divisor of $n$ is also a divisor of $a$. In particular determine
the nilpotent elements of $\Z/72\Z$ explicitly.}
\begin{proof}
$(\Rightarrow)$ Let $a\in \Z$ and  $\bar{a}\in\Z/n\Z$ is a
nilpotent, and let $p$ be a prime divisor of $n$ (i.e. $p\big|n$).
Since $a^k\equiv 0\mod n$, then $p\big|a^k$, hence $p\big|a$,
since $p$ is a prime number.

$(\Leftarrow)$  Suppose $n=p_1^{\alpha_1}p_2^{\alpha_2}...p_k^{\alpha_k}$, 
then $a = p_1^{\gamma_1}p_2^{\gamma_2}...p_k^{\gamma_k}$ with
$1 \leq \gamma_i \le \alpha_i$ for each prime $p_i$.  Let $m$ be the maximum
of the $\alpha_i$.  Then $n \divides a^m$ and thus, $a^m \equiv 0 \mod n$.  
It follows that $\bar{a}$ is a nilpotent element of $\Z/n\Z$.
\end{proof}

The nilpotent elements of $\Z/72\Z$ are
$0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66$.

\emph{(c) Let $R$ be a ring of functions from nonempty set $X$ to
a field $F$. Prove that $R$ contains no nonzero nilpotent element
}
\begin{proof}
Let $f$ be a nonzero nilpotent element in $R$ then there exist
$x\in X$ s.t. $f(x)\ne0$. Since $f$ is nilpotent, then
$f^k\equiv0$ for some $k\in \Z^+$, then $f^k(x)=[f(x)]^k=0$ from
which it follows that $f(x)=0$, but this is a contradiction. Hence
$R$ contains no nonzero nilpotent element.
\end{proof}


\item[14.]\emph{Let $x$ be a nilpotent of the commutative ring
$R$}

\emph{(a) Prove that $x$ is either zero or zero divisor.}
\begin{proof}
If $x=0$ then we are done.

Let $x\ne0$ and let $k$ be minimal element in $\Z^+$ s.t. $x^k=0$,
since $x$ is a nilpotent, then $xx^{k-1}=0$ where $x^{k-1}\ne0$.
Therefore $x$ is a zero divisor.
\end{proof}

\emph{(b) Prove that $rx$ is nilpotent for all $r\in R$.}
\begin{proof}
$(rx)^k=r^kx^k=r^k0=0$ hence $rx$ is nilpotent for all $r\in R$.
\end{proof}

\emph{(c) Prove that $1+x$ is a unit in $R$.}
\begin{proof}
$$(1+x)(1-x+x^2-x^3+...+(-1)^{m-1}x^{m-1})=1$$
\end{proof}

\emph{(d)  Deduce that the sum of nilpotent element and a unit is
a unit.}
\begin{proof}
Let $u+x$ be the sum of nilpotent element $x$ and a unit $u$. Then
$u+x=u(1+u^{-1}x)$, from part (a) $u^{-1}x$ is a nilpotent, by
part (b) $1+u^{-1}x$ is a unit, so multiplication of two unit
elements of $R$ is equal to a unit element in $R$.
\end{proof}

\item[18.]  The set $S=\{(r,r)|r\in R\}$ is a subring of $R\times R$.

% display your last name 
\begin{proof}(Gillispie) By definition $S\subset R\times R$.\\
Pick $a,b\in R$. So $(a,a)+(-b,-b)=(a-b,a-b)\in S$ and so $(S,+)\le(R,+)$.\\
$S$ inherits the associative and distributive laws from $R\times R$.\\
Finally $(a,a)\cdot(b,b)=(ab,ab)\in R$ and so $R$ is closed under
multiplication and $S$ is a subring of $R$.
\end{proof}

\end{itemize}

\section*{Chapter 7.2}

\begin{itemize}


\item[2.]  Follow hint in book.  

\item[3.] Define the set $R[[x]]$ of formal power series in the indeterminate $x$ with coefficients from $R$ to be all formal infinite $$\sum_{n=0}^\infty a_nx^n = a_0+a_1x+a_2x^2+a_3x^3+\ldots.$$
    Define addition and multiplication of power series in the same way as for power series with real or complex coefficients i.e., extend polynomial addition and multiplication to power series as though they were "polynomials of infinite degree":
\begin{align*}
\sum_{n=0}^\infty a_nx^n &+\sum_{n=0}^\infty b_nx^n  = \sum_{n=0}^\infty (a_n+b_n)x^n\\
\sum_{n=0}^\infty a_nx^n & \times\sum_{n=0}^\infty b_nx^n  = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_kb_{n-k}\right)x^n.
\end{align*}

\begin{itemize}

\item[a.] Prove that $R[[x]]$ is a commutative ring with 1.

\begin{proof}(Hazlett)
Note, a technical proof that $+$ and $\times$ are associative can be written.  We will exclude these.  Let $p,q \in R[[x]]$ where
$$p = \sum_{n=0}^\infty a_nx^n, q = \sum_{n=0}^\infty b_nx^n.$$
Then
$$p + q = \sum_{n=0}^\infty a_nx^n +\sum_{n=0}^\infty b_nx^n  = \sum_{n=0}^\infty (a_n+b_n)x^n.$$
Each $a_n+b_n\in R$ so $p+q \in R[[x]]$.
Let $0 = \sum_{n=0}^\infty c_nx^n$ where $c_n =0_R$ for all $n$.  Then
$$p+0 = \sum_{n=0}^\infty (a_n+ 0_R)x^n = \sum_{n=0}^\infty a_nx^n = p.$$
Similarly $0+p=0$.  Set $-p = \sum_{n=0}^\infty -a_nx^n$.  We know that $-p\in R[[x]]$ since each $-a_n \in R$ because $R$ is a ring.  Hence
$$p+(-p) = \sum_{n=0}^\infty (a_n-a_n)x^n = \sum_{n=0}^\infty 0_Rx^n = 0.$$
This implies every element of $R[[x]]$ has an additive inverse.
It is clear that since $R$ is an abelian group that $R[[x]]$ is also abelian.
Consequently we have $R[[x]]$ is an abelian group under addition.
Select $r \in R[[x]]$ where the coefficients of $r$ are $c_n$.
Then
\begin{align*}
p(q+r) & = \sum_{n=0}^\infty a_nx^n \times  \left(\sum_{n=0}^\infty b_nx^n + \sum_{n=0}^\infty c_nx^n\right)\\
& = \sum_{n=0}^\infty a_nx^n \times  \sum_{n=0}^\infty (b_n+c_n)x^n \\
& = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_k(b_{n-k}+c_{n-k})\right)x^n\\
& = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_kb_{n-k}\right)x^n+\sum_{n=0}^\infty\left(\sum_{k=0}^n a_kc_{n-k}\right)x^n\\
& = pq+pr.
\end{align*}
We conclude that multiplication is distributive.  We also can see that multiplication is commutative by noting that the coefficient of $x^n$ in $pq$ is $\sum_{k=0}^na_kb_{n-k} = a_0b_n + a_1b_{n-1}+a_2b+{n-2} + \ldots + a_{n-1}b_1 + a_nb_0 = b_0a_n + b_1a_{n-1} + \ldots +b_{n-1}a_1+b_na_0$, which is the coefficient of $x^n$ in $qp$.  Denote $1 = \sum_{k=0}^\infty d_nx^n$ where $d_0 = 1_R$ and $d_n = 0$ for $n > 0$.  Then
$$p\cdot1 = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_kd_{n-k}\right)x^n = \sum_{n=0}^\infty a_nx^n = p.$$  Since $R[[x]]$ is commutative we have $1$ is the multiplicative identity.  Therefore $R[[x]]$ is a commutative ring with a 1.
\end{proof}

\item[b.] Show that $1-x$ is a unit in $R[[x]]$ with inverse $1+x+x^2+\ldots$.

\begin{proof}(Hazlett)
Consider $(1-x)(1+x+x^2+\ldots)$.  Note, this is equal to $1(1+x+x^2+\ldots) - x(1+x+x^2+\ldots) = 1+x+x^2+\ldots - x - x^2- x^3 - \ldots = 1 + (x-x) + (x^2-x^2) + \ldots = 1$.  Therefore $(1-x)(1+x+x^2+\ldots) = 1$ and $(1-x)$ is a unit with inverse $1+x+x^2+\ldots$.
\end{proof}

\item[c.] Prove that $\sum_{n=0}^\infty a_nx^n$ is a unit in $R[[x]]$ if and only if $a_0$ is a unit in $R$.

\begin{proof}(Hazlett)
Suppose $\sum_{n=0}^\infty a_nx^n$ is a unit.  Then there exists some $\sum_{n=0}^\infty b_nx^n$ where $$\sum_{n=0}^\infty a_nx^n \times \sum_{n=0}^\infty b_nx^n = 1.$$  Note, the constant term of this product is $a_0b_0 = 1$.  Therefore $a_0$ is a unit with inverse $b_0$.  Assume instead then that $a_0$ is a unit.  Set $a_0\inv = c_0$.  We want to find a series $\sum_{n=0}^\infty c_nx^n$ such that $$\sum_{n=0}^\infty a_nx^n  \times\sum_{n=0}^\infty c_nx^n = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_kc_{n-k}\right)x^n = 1.$$  Specifically we need $a_0c_0 = 1$, which we have from above, and $\sum_{k=0}^n a_kc_{n-k} = 0$ for $n \geq 1$.  We will proceed by induction on $n$.  For $n = 1$ we need $a_0c_1+a_1c_0 = 0$.  Then $c_1 = -a_0\inv a_1c_0$, which exists because $a_0 \inv$ exists.  So suppose $c_\ell$ exists for all $\ell$ such that $1 \leq \ell < n$.  Set $c_n = -a_0\inv(a_1c_{n-1}+a_2c_{n-2}+\ldots+a_nc_0)$.  Then
\begin{align*}
\sum_{k=0}^n a_kc_{n-k} & = a_0c_n + a_1c_{n-1}+a_2c_{n-2}+\ldots+a_nc_0\\ & = -a_0a_0\inv(a_1c_{n-1}+a_2c_{n-2}+\ldots+a_nc_0)+ a_1c_{n-1}+a_2c_{n-2}+\ldots+a_nc_0 \\ & = 0.
\end{align*}
Consequently we can find a series $\sum_{n=0}^\infty c_nx^n$ where $$\sum_{n=0}^\infty a_nx^n  \times\sum_{n=0}^\infty c_nx^n = \sum_{n=0}^\infty\left(\sum_{k=0}^n a_kc_{n-k}\right)x^n = 1.$$  Therefore $\sum_{n=0}^\infty a_nx^n$ is a unit.
\end{proof}

\end{itemize}

\item[8.] Let $S$ be any ring and let $n\geq 2$ be an integer.  Prove that if $A$ is any strictly upper triangular matrix in $M_n(S)$ then $A^n = 0$.

\begin{proof}(Hazlett)
We claim that in the matrix $B = (b_{i,j}) = A^\ell$ we have $b_{i,j} = 0$, 
if $i < j + \ell - 1$.  We proceed by induction.  The case for $\ell=1$ is clear from the fact that $A$ is upper triangular.  Let $\ell=2$.  Hence, $B = A^2$.  Then $b_{i,j} = \sum_{k=1}^na_{i,k}a_{k,j}$.  Thus $b_{i,j} = 0$ for $i< k$ and $k < j$.  This implies $b_{i,j} = 0$ for $i < j+1 = j + \ell -1$.  So suppose that $B = A^\ell$ and $b_{i,j} = 0$ for $i < j+\ell - 1$.  Set $C = A^{\ell+1}$.  Thus $C = AA^{\ell} = AB$.  Then $c_{i,j} = \sum_{k=1}^na_{i,k}b_{k,j}$.  Hence $c_{i,j} = 0$ for $i < k$ and $k < j+\ell - 1$.  So $c_{i,j} = 0$ for $i < j + \ell = j + (\ell +1) - 1$, as desired.  Let $D = A^n$.  Then $d_{i,j} = 0$ for all $i < j + n - 1$.  Since $D$ is a $n\times n $ matrix we have $i < j + n - 1$ for all $i,j$.  Therefore $D = 0$.  
\end{proof}


\item[9.] Let $\alpha = r+r^2-2s$ and $\beta = -3r^2 + rs$ be the two elements of the integral group ring $\Z D_8$ described in this section.  Compute the following elements of $\Z D_8$:

\begin{itemize}

\item[a.] $\beta\alpha = -3 -2r - 3r^3+s+6r^2s +r^3s$

\item[b.] $\alpha^2 = 5+r^2+2r^3-2rs-4r^2s-2r^3s$

\item[c.] $\alpha\beta - \beta\alpha = 2r-2r^3-s+r^2s$

\item[d.] $\beta\alpha\beta = 15r+10r^2+7r^3-21s-6rs-5r^2s$

\end{itemize}

\end{itemize}

\section*{Chapter 7.3}

\begin{itemize}

%%%%%%%%%%%%%%%%%%%%
\item[1.] Let $R$ be a ring with identity $1 \neq 0$. Prove that the rings $2\Z$ and $3\Z$ are not isomorphic.

\begin{proof} Suppose there is an isomorphism $\varphi: 2\Z \rightarrow 3\Z$. Let $\varphi(2) = 3m$, where $m \in \Z^*$ (If $m = 0$, $\varphi(2) = 3 \times 0 = 0$ then $\varphi(2\Z) = \left\{0\right\}$, which contradicts our assumption $\varphi$ is an isomorphism).

Consider $\varphi(4)$, $\varphi(4) = \varphi(2+2) = 3m + 3m = 6m$. 
On the other hand, $\varphi(4) = \varphi(2 \times 2) = 3m \times 3m = 9m^2$.  While $m \in \Z^*$, $6m \neq 9m^2$. Thus $\varphi$ is not an isomorphism, and the rings $2\Z$ and $3\Z$ are not isomorphic.
\end{proof}

%%%%%%%%%%%%%%%%%%%%
\item[4.] Find all ring homomorphisms from $\Z$ to $\Z/30\Z$. In each case describe the kernel and the image.

\begin{proof} Let's assume there is a homomorphism $\varphi$ from $\Z$ to $\Z/30\Z$.  Since $1$ generates the entire $\Z$ (by addition) let's consider its homomorphic image in $\Z/30\Z$. Let $\varphi(1) = \overline{a}$ for some $\overline{a} \in \Z/30\Z$, then for any $m \in \Z$ we have $\varphi(m) = m \cdot \overline{a} = \overline{am}$. It is easy to check $\varphi$ preserves the operation of addition. To ensure $\varphi$ is a ring homomorphism, take any $m, n \in \Z$, we need to satisfy $\varphi(mn) = \varphi(m)\varphi(n)$, i.e., $\overline{amn} = \overline{am}\times \overline{an} = \overline{a^2mn}$.  This forces $\overline{a} = \overline{a^2}$. In $\Z/30\Z$,  $\overline{0}, \overline{1}, \overline{6}, \overline{10}, \overline{15}, \overline{16}, \overline{21}, \overline{25}$ satisfies this relationship for $\overline{a}$. Thus the homomorphisms ($\varphi$) from $\Z$ to $\Z/30\Z$ are:
  
 \begin{tabular}{lll}
    Homomorphism & Kernel & Image in $\Z/30\Z$\\
		\hline
    $\varphi(m) = \overline{0}$ & $\Z$ & $\left\langle \overline{30}\right\rangle = \left\langle \overline{0}\right\rangle$\\  
    $\varphi(m) = \overline{m}$ & $30\Z$ & $\left\langle \overline{1} \right\rangle = \Z/30\Z$\\
    $\varphi(m) = \overline{6m}$ & $5\Z$ & $\left\langle \overline{6} \right\rangle$\\
    $\varphi(m) = \overline{10m}$ & $3\Z$ & $\left\langle \overline{10} \right\rangle$\\
    $\varphi(m) = \overline{15m}$ & $2\Z$ & $\left\langle \overline{15} \right\rangle$\\
    $\varphi(m) = \overline{16m}$ & $15\Z$ & $\left\langle \overline{16} \right\rangle = \left\langle \overline{2} \right\rangle$\\
    $\varphi(m) = \overline{21m}$ & $10\Z$ & $\left\langle \overline{21} \right\rangle = \left\langle \overline{3} \right\rangle$\\
    $\varphi(m) =\overline{25m}$ & $6\Z$ & $\left\langle \overline{25} \right\rangle = \left\langle \overline{5} \right\rangle$\\
   \end{tabular}
\end{proof}

\item[5.] Describe all ring homomorphisms from the ring $\Z \times \Z$ to $\Z$. In each case describe the kernel and the image. 

\begin{proof} Let's assume there is a homomorphism $\varphi$ from $\Z \times \Z$ to $\Z$.  Since $\Z \times \Z = \left\langle (1,0),(0,1) \right\rangle$ by addition, let's consider the homomorphic images of $(1,0)$ and $(0,1)$ in $\Z$. 

Let $\varphi((1,0)) = n$ for some $n \in \Z$, this extends to a homomorphism from $\left\langle(1,0)\right\rangle$ to $n\Z$. Since $\left\langle(1,0)\right\rangle \cong \Z$, and $\Z$ is homomorphic to $n\Z$ only when $n = 0,1$, we can determine all the homomorphisms from $\left\langle(1,0)\right\rangle$ to $\Z$, namely, $\varphi(s,0)=0$ and $\varphi(s,0)=s$, where $s \in \Z$.  Similarly, the homomorphisms from $\left\langle(0,1)\right\rangle$ to $\Z$ are $\varphi(0,r)=0$ and $\varphi(0,r)=r$, where $r \in \Z$.

For any element $(s, r) \in \Z \times \Z$, $\varphi(s, t) = s \cdot \varphi(1, 0) + t \cdot \varphi(0, 1)$.  Looking through all (four) combinations of the homomorphic images of $\left\langle(1,0)\right\rangle$ and $\left\langle(0,1)\right\rangle$ we gladly find three of them still remain to be ring homomorphisms. They are:

  \begin{tabular}{lll}
    Homomorphism & Kernel & Image in $\Z$ \\
		\hline
    $\varphi((s,r)) = 0$ & $\Z \times \Z$ & $\left\{0\right\}$\\		
    $\varphi((s,r)) = s$ & $\left\{0\right\} \times \Z$ & $\Z$\\
    $\varphi((s,r)) = r$ & $\Z \times \left\{0\right\}$ & $\Z$\\
  \end{tabular}
 \end{proof}

\item[10.]  Decide which of the following are ideals of the ring $\Z[x]$.

\begin{proof} (Allman)

\begin{itemize}

\item[(a)] The set of all polynomials whose constant term is a multiple of $3$.
YES.

\item[(b)] The set of all polynomials whose coefficient of $x^2$ is a multiple of $3$.
NO.  Counter-example: $f(x) = 3x^2 + x$ would be in this set, but
$f(x) \cdot x = 3x^3 + x^2$ is not.  Thus, the `absorption property' fails.

\item[(c)] The set of all polynomials whose constant term, coefficient of $x$,
and coefficient of $x^2$ are zero.  YES.  This is the principal ideal generated
by $x^2$, i.e. $I = (x^2)$.

\item[(d)] $\Z[x^2]$.  NO.  The absorption property fails.  For instances,
$2 \in \Z[x^2]$, but $2x$ is not.

\item[(e)] The set of polynomials whose coefficients sum to zero.
YES.  This is a fancy way of saying that $1$ is a root.  This set is
simply the principal ideal $(x-1)$.

\item[(f)] The set of polynomials $p(x)$ such that $p'(0) = 0$, where
$p'(x)$ is the usual first derivative of $p(x)$ with respect to $x$.
NO.  If $p'(0) = 0$, then $x$ is a root of the derivative of $p(x)$,
and any such $p(x)$ has no linear term.  That is, if $p(x) = \sum^k_{i=0}
a_i x^i$, then $a_1 = 0$.  This can not be an ideal, since 
$p(x) = x^2 +1$ is in this set, but $xp(x) = x^3 + x$ does not
satisfy $p'(0) = 0$.\end{itemize}
\end{proof}

\item[17.]
Let $R$ and $S$ be nonzero rings with identity and denote their respective identities by $1_R$ and $1_S$. Let $\varphi: R \rightarrow S$ be a nonzero homomorphism of rings.

(a) Prove that if $\varphi(1_R) \neq 1_S$ then $\varphi(1_R)$ is a zero divisor in $S$. Deduce that if $S$ is an integral domain then every ring homomorphism from $R$ to $S$ sends the identity of $R$ to the identity of S.

\begin{proof}
For $\varphi$ to be nonzero homomorphism we must have $\varphi(1_R) \neq 0_S$, because if $\varphi(1_R) = 0_S$ then $\forall \ r \in R, \ \varphi(r) = \varphi(1_R \times r)= \varphi(1_R) \times \varphi(r) = 0$. Consider $$1_S \times \varphi(1_R) = \varphi(1_R) =  \varphi(1_R \times 1_R ) = \varphi(1_R) \times \varphi(1_R) \Rightarrow (1_S - \varphi(1_R)) \times \varphi(1_R) = 0$$ Now $\varphi(1_R) \neq 0$ and $\varphi(1_R) \neq 1_S$, then $\varphi(1_R)$ must be a zero divisor.

Since integral domains do not allow zero divisors, if $S$ is an integral domain then $\varphi(1_R) = 1_S$, i.e., every ring homomorphism from $R$ to $S$ sends the identity of $R$ to the identity of S.
\end{proof}

(b) Prove that if $\varphi(1_R) = 1_S$ then $\varphi(u)$ is a unit in $S$ and that $\varphi(u^{-1}) = \varphi(u)^{-1}$ for each unit $u$ of $R$.

\begin{proof}
Let $u$ be a unit in $R$ $\Rightarrow \exists \ u^{-1}$ such that $uu^{-1} = u^{-1}u =1$. Consider
$1_S = \varphi(1_R) = \varphi(uu^{-1}) = \varphi(u)\varphi(u^{-1})$, therefore $\varphi(u^{-1}) = \varphi(u)^{-1}$ and $\varphi(u)$ is a unit in $S$.
\end{proof}


\item[18.] 
\begin{itemize}
\item[a.] If $I$ and $J$ are ideals of $R$ prove their intersection $I \cap J$ is also an ideal of $R$.
\begin{proof}(Baggett) \ From Exercise 2.1.10(a), ($I \cap J$, +) is a subgroup of ($R$, +) since ($I$, +) 
and ($J$, +) are subgroups of ($R$, +). Take any element $r \in R$ and any element $k \in I \cap J$.
Then $k \in I$ and $k \in J$. Since $I$ and $J$ are ideals, $rk \in I$ and $rk \in J$. Thus, $rk \in I \cap J$.
Thus, $I \cap J$ is closed under left multiplication by elements of $R$. Similarly, $I \cap J$ is closed
under right multiplication by elements of $R$. Hence, $I \cap J$ is an ideal of $R$.
\end{proof}

\item[b.] Prove that the intersection of an arbitrary nonempty collection of ideals is again an ideal
(do not assume the collection is countable).
\begin{proof} (Baggett) \ Let $\{ I_\lambda \}_{\lambda \in \Lambda}$ be an arbitrary nonempty collection
of ideals of a ring $R$. From Exercise 2.1.10(b), ($\bigcap_{\lambda \in \Lambda} I_\lambda$, +) is a subgroup 
of ($R$, +) since for every $\lambda \in \Lambda$, ($I_\lambda$, +) is a subgroup of ($R$, +). Take any element
$k \in \bigcap_{\lambda \in \Lambda} I_\lambda$. Then $k \in I_\lambda$ for every $\lambda \in \Lambda$. Since
$I_\lambda$ is an ideal, $rk \in I_\lambda$ for every $\lambda \in \Lambda$. Thus, 
$rk \in \bigcap_{\lambda \in \Lambda} I_\lambda$. Hence, $\bigcap_{\lambda \in \Lambda} I_\lambda$ is closed
under left multiplication by elements of $R$. Similarly, $\bigcap_{\lambda \in \Lambda} I_\lambda$ is closed
under right multiplication by elements of $R$. Thus, $\bigcap_{\lambda \in \Lambda} I_\lambda$ is an ideal of $R$.
\end{proof}
\end{itemize}


\item[22.] Let $a$ be an element of the ring $R$.
\begin{itemize}
\item[a.] Prove that $\{x \in R \ | \ ax = 0\}$ is a right ideal and $\{y \in R \ | \ ya = 0\}$ is a left ideal
(called respectively the right and left annihilators of $a$ in $R$).
\begin{proof} (Baggett) \ Let $I = \{x \in R \ | \ ax = 0\}$ and $J = \{y \in R \ | \ ya = 0\}$. Take any two elements
$x,y \in I$. Then $x - y \in I$ since $a(x - y) = ax - ay = 0$. Thus, ($I$, +) is a subgroup of ($R$, +).
Similarly, ($J$, +) is a subgroup of ($R$, +). Take any element $r \in R$ and any element $x \in I$. Then 
$xr \in I$ since $a(xr) = (ax)r = 0 \cdot r = 0$. Thus, $Ir \subseteq I$ and $I$ is a right ideal. Similarly,
take any element $r \in R$ and any element $y \in J$. Then $ry \in J$ since $(ry)a = r(ya) = r \cdot 0 = 0$. 
Thus, $rJ \subseteq J$ and $J$ is a left ideal.
\end{proof}

\item[b.] Prove that if $L$ is a left ideal of $R$ then $\{x \in R \ | \ xa = 0$ for all $a \in L\}$ is a two-sided
ideal (called the left annihilator of $L$ in $R$).
\begin{proof} (Baggett) \ Let $I = \{x \in R \ | \ xa = 0$ for all $a \in L\}$. Take any two elements $x,y \in I$. 
Then $x - y \in I$ since for any $a \in L$, $(x - y)a = xa - ya = 0$. Thus, ($I$, +) is a subgroup of ($R$, +).
Take any element $r \in R$ and any element $x \in I$. Then $rx \in I$ since for any $a \in L$, 
$(rx)a = r(xa) = r\cdot0 = 0$. Thus, $rI \subseteq I$. Since $L$ is a left ideal of $R$, $ra \in L$ for every
$a \in L$. Let $ra = a' \in L$. Then $xr \in I$ since $(xr)a = x(ra) = xa' = 0$. Thus, $Ir \subseteq I$. Therefore,
$I$ is a two-sided ideal.
\end{proof}
\end{itemize}

\item[24.] Let $\varphi: R \rightarrow S$ be a ring homomorphism.
\begin{itemize}
\item[a.] Prove that if $J$ is an ideal of $S$ then $\varphi^{-1}(J)$ is an ideal of $R$. Apply this to the special
case when $R$ is a subring of $S$ to deduce that if $J$ is an ideal of $S$ then $J \cap R$ is an ideal of $R$.
\begin{proof} (Baggett) \ Since ($J$, +) is a subgroup of ($S$, +) and $\varphi$ is a group homomorphism under addition,
 ($\varphi^{-1}(J)$, +) is a subgroup of ($R$, +) from Exercise 3.1.1. Take any element $r \in R$ and any element
 $x \in \varphi^{-1}(J)$. Then $rx \in \varphi^{-1}(J)$ since $\varphi(x) \in J$ and 
 $\varphi(rx) = \varphi(r)\varphi(x) \in J$. Similarly, $xr \in \varphi^{-1}(J)$. Thus, 
 $r\varphi^{-1}(J) \subseteq \varphi^{-1}(J)$ and $\varphi^{-1}(J)r \subseteq \varphi^{-1}(J)$. Therefore,
 $\varphi^{-1}(J)$ is an ideal of $R$.\\ 
 \ \ In particular, suppose $R$ is a subring of $S$ and let $\varphi$ be the inclusion homomorphism, i.e. 
 $\varphi: R \rightarrow S$ maps $r$ to $r$. Let $J$ be an ideal of $S$. Then 
 $\varphi^{-1}(J) = \{r \in R \ | \ \varphi(r) \in J \} = \{r \in R \ | \ r \in J \} = J \cap R$ is an
 ideal of $R$.
\end{proof}


\item[b.] Prove that if $\varphi$ is surjective and $I$ is an ideal of $R$ then $\varphi(I)$ is an ideal of $S$.
Give an example where this fails if $\varphi$ is not surjective.
\begin{proof} (Baggett) \ Since $I$ is an ideal of $R$, ($I$, +) is a subgroup of ($R$, +). Since $\varphi$ is a
group homomorphism under addition, ($\varphi(I)$, +) is a subgroup of ($S$, +). Take any element $y \in \varphi(I)$
and any element $s \in S$. Since $\varphi$ is surjective, there exists elements $x \in I$ and $r \in R$ such that
$y = \varphi(x)$ and $s = \varphi(r)$. Then $sy \in \varphi(I)$ since 
$rx \in I$ and $sy = \varphi(r)\varphi(x) = \varphi(rx) \in \varphi(I)$. Similarly, $ys \in \varphi(I)$ as well. Thus,
$s\varphi(I) \subseteq \varphi(I)$ and $\varphi(I)s \subseteq \varphi(I)$. Therefore, $\varphi(I)$ is an ideal of $S$.\\ 
\ \ Let $R = \Z$, $S = \mathbb{Q}$, and $\varphi:R \rightarrow S$ by $\varphi(n) = n$. Then $\varphi$ is not surjective. Let
$I = 2\Z$. Then $2\Z$ is an ideal of $\Z$; however, $2\Z$ is not an ideal of $\mathbb{Q}$. Take $2 \in 2\Z$ and $\frac{1}{2} 
\in \mathbb{Q}$. Then $\frac{1}{2} \cdot 2 = 1 \notin 2\Z$. Thus, $\varphi(I) = 2\Z$ is not an ideal of $\mathbb{Q}$.
\end{proof}
\end{itemize}

\end{itemize}

\section*{Chapter 7.4}

\begin{itemize}

\item [4.] Assume $R$ is commutative. Prove that $R$ is a field if and
only if $0$ is a maximal ideal. 

\begin{proof}[Proof (Granade)]
We shall show each direction in turn.
\begin{itemize}
\item [$\Rightarrow$] Suppose that $R$ is a field. Then, let $I$ be
an ideal of $R$ containing $r\ne0$. Since $R$ is a field, $r^{-1}$
exists, and so by the absorbing property of $I$, $1=r^{-1}r\in I$.
But then, for all $s\in R$, we have that $s=s\cdot1\in I$ by the
same absorbing property. Hence, $I=R$.
\item [$\Leftarrow$] Suppose that $0$ is a maximal ideal in $R$. Then,
fix $r\in R\backslash\left\{ 0\right\} $ and consider the ideal $\left(r\right)$.
Since $0$ is a maximal ideal, we have that $\left(r\right)=R$. In
particular, $1\in R$, and so there exists some element $s\in R$
such that $sr=1$. But then, this gives that $s=r^{-1}$, demonstrating
that every element in $R\backslash\left\{ 0\right\} $ has an inverse.
We conclude that $R$ is a field.
\end{itemize}
Since we have shown each direction, we are done.
\end{proof}

\newpage

\item [5.] Prove that if $M$ is an ideal such that $R/M$ is a field,
then $M$ is a maximal ideal (do not assume that $R$ is commutative.)


\begin{proof}[Proof (Granade)]
Let $M$ be an ideal of $R$ such that $R/M$ is a field. Then, by
the Fourth Isomorphism Theorem for Rings, there exists a one-to-one
correspondence between ideals $A\supseteq M$ of $R$ and $A/M$ of
$R/M$. But then, by the previous problem, since $R/M$ is a field,
it admits only two ideals: $R/M$ and $\left\{ 0_{R/M}\right\} $.
Thus, there exists exactly two ideals of $R$ which contain $M$.
We conclude that since $M$ and $R$ are both ideals of $R$ containing
$M$, $M$ is maximal in $R$.
\end{proof}


\item[11.] Assume $R$ is commutative. Let $I$ and $J$ be ideals of $R$ and assume $P$ is a prime ideal of $R$ that contains $IJ$ (for example, if $P$ contains $I \cap J$). Prove either $I$ or $J$ is contained in $P$. 
\begin{proof} (Bastille) \ By definition $IJ=\{i_1j_1+i_2j_2+\cdots + i_nj_n \ | \ i_k \in I, j_k \in J, n \in \Z^{+} \}$. If $J \subseteq P$ then we are done so assume $J$ is not contained in $P$. Then there exists $j \in J$ such that $j \notin P$. Let $i \in I$. Then $ij \in P$ since $\{ij \ | \ i \in I \} \subseteq IJ \subseteq P$, but then since $P$ is a prime ideal, we must have $i \in P$. Therefore $I \subseteq P$. Thus in every case, either $I$ or $J$ is contained in $P$. 
\end{proof}

\item[13.] Let $\varphi:R \to S$ be a homomorphism of commutative rings.
\begin{enumerate}
\item[(a)] Prove that if $P$ is a prime ideal of $S$ then either $\varphi^{-1}(P)=R$ or $\varphi^{-1}(P)$ is a prime ideal of $R$. Apply this to the special case when $R$ is a subring of $S$ and $\varphi$ is the inclusion homomorphism to deduce that if $P$ is a prime ideal of $S$ then $P \cap R$ is either $R$ or a prime ideal of $R$.
\item[(b)] Prove that if $M$ is a maximal ideal of $S$ and $\varphi$ is surjective then $\varphi^{-1}(M)$ is a maximal ideal of $R$. Give an example to show that this need not be the case if $\varphi$ is not surjective.
\item[(a)]
\begin{proof} (Bastille) 
 Let $P$ be a prime ideal of $S$. By Exercise 7.3.24(a), since $P$ is an ideal of $S$, then $\varphi^{-1}(P)$ is an ideal of $R$. If $\varphi^{-1}(P)=R$ then we are done. So assume $\varphi^{-1}(P) \neq R$ and let $ab \in \varphi^{-1}(P)$. Then $\varphi(ab)=\varphi(a) \cdot \varphi(b) \in P$ and so either $\varphi(a)$ or $\varphi(b)$ is in $P$ since $P$ is a prime ideal. Hence by definition of $\varphi^{-1}(P)$, either $a$ or $b$ is in $\varphi^{-1}(P)$. Therefore, $\varphi^{-1}(P)$ is a prime ideal of $R$.
\end{proof}
\underline{Remark} Let $R$ be a subring of $S$, let $\varphi$ be the inclusion homomorphism, i.e. $\varphi: R \to S$ is defined by $\varphi(r)=r$, and let $P$ be a prime ideal of $S$. By definition,
$$\varphi^{-1}(P)= \{r \in R \ | \ \varphi(r) \in P \}= \{ r \in R \ | \ r \in P \}= P \cap R.$$
Hence by the Proposition above, $P \cap R= R$ or $P \cap R$ is a prime ideal of $R$.
\item[(b)]
\begin{proof} (Bastille) Let $M$ be a maximal ideal of $S$ and let $\varphi$ be a surjective homomorphism from $R$ to $S$. Since $M$ is maximal, it is prime, and so by part (a), $\varphi^{-1}(M)$ is either $R$ or a prime ideal of $R$. We claim that since $\varphi$ is surjective we cannot have $\varphi^{-1}(M)=R$. Assume to the contrary that $\varphi^{-1}(M)=R$. Then because $\varphi$ is surjective,
$$M=\varphi(\varphi^{-1}(M))=\varphi(R)=S.$$ 
Yet $M$ is a maximal ideal of $S$ so $M \neq S$ and we have reached a contradiction. So we can now proceed assuming $\varphi^{-1}(M)$ is a prime ideal of $R$. Define the following map:
\begin{align*}
 \sigma &: R \to S/M \\
 &\sigma(r)= \varphi(r)+M.
\end{align*}
We verify that $\sigma$ is well-defined and a ring homomorphism since
\begin{align*}
\forall \ r_1,r_2 \in R: \quad \sigma(r_1+r_2) 	&= \varphi(r_1+r_2)+M \\
																								&= \varphi(r_1)+\varphi(r_2)+M \quad \text{ since $\varphi$ is a ring homomorphism} \\
																								&=(\varphi(r_1)+M)+(\varphi(r_2)+M) \quad \text{ since $M$ is an ideal} \\
																								&=\sigma(r_1)+\sigma(r_2), \\
\text{ and } \quad 							\sigma(r_1r_2)	&=\varphi(r_1r_2)+M \\
																								&= \varphi(r_1)\varphi(r_2)+M	\quad \text{ since $\varphi$ is a ring homomorphism} \\
																								&=(\varphi(r_1)+M)(\varphi(r_2)+M) \quad \text{ since $M$ is an ideal} \\
																								&=\sigma(r_1)\sigma(r_2). \\																											
\end{align*}
Furthermore, $$\ker \sigma = \{ r \in R \ | \ \varphi(r)+M=M \}=\{ r \in R \ | \ \varphi(r) \in M \}= \varphi^{-1}(M),$$ and the map is surjective since $\varphi$ is surjective. Therefore, by the First Isomorphism Theorem, 
$$ R/\varphi^{-1}(M) \cong S/M. $$
But because $M$ is maximal in $S$, $S/M$ is a field. Therefore, $R/\varphi^{-1}(M)$ is a field, and consequently $\varphi^{-1}(M)$ is maximal.
\end{proof}
\underline{Remark} This need not be true if $\varphi$ is not surjective. Let $M$ be a maximal ideal of $S$, let $R=M$ and define $\varphi$ as the inclusion map, $\varphi: R \to S$, $\varphi(r)=r$. Then $\varphi^{-1}(M)=R$, which is not a maximal ideal of $R$ since it equals $R$. Concretely, consider $\varphi: 2\Z \to \Z$, $\varphi(k)=k$, $M=2 \Z$, a maximal ideal in $\Z$, yet since $\varphi^{-1}(2 \Z)=2\Z$, it is not a maximal ideal of $2\Z$.
\end{enumerate}

\item[25.] Assume $R$ is commutative and for each $a \in R$ there is an integer $n > 1$ such that $a^n = a$. Prove that every prime ideal of $R$ is maximal. 

\begin{proof}(Lawless)
Let $P$ be a prime ideal of $R$. Since $R$ is commutative, then we know $R/P$ will be an integral domain. We will show $R/P$ is a field. 

Let $a + P \neq 0 + P$, and fix $n \in \Z^+$ so that $a^n = a$. Notice
$$(a+P)(a^{n-1}+P) = a^n + P = a+P.$$
Therefore,
\begin{align*}
0 + P &= (a+P)(a^{n-1}+P)-(a+P) \\
&= (a+P)[(a^{n-1}+P)-(1+P)] \\
\end{align*}
Since $P$ is a prime ideal, and we assumed $a+P \neq P$, then $(a^{n-1}+P)-(1+P) = P$. Thus, $a^{n-1}+P = 1 + P$. Therefore $(a+P)(a^{n-2}+P) = 1 + P$. Therefore, $a+P$ is a unit, and so $R/P$ is a field. So $P$ is a maximal ideal. 
\end{proof}


\item[27.] Let $R$ be a commutative ring with unity. Prove that if $a$ is a nilpotent element of $R$, then $1 - ab$ is a unit for all $b \in R$. 

\begin{proof} (Lawless)
Let $a \in R$ satisfy $a^n = 0$. Then for any $b \in R$,
\begin{align*}
(1-ab)(1 + ab + (ab)^2 + \ldots + (ab)^{n-1}) &= 1 - (ab)^n \\
&= 1 - a^nb^n \text{ (since $R$ is commutative) } \\
&= 1.
\end{align*}
Therefore, $(1-ab)^{-1} = \sum_{k=0}^{n-1} (ab)^k$, and so $(1-ab)$ is a unit in $R$. 

\end{proof}


\end{itemize}

\end{document}
